#!/usr/bin/env python# coding: utf-8# In[1]:from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformerimport osimport reimport spacyfrom spacy.lemmatizer import Lemmatizerfrom nltk import PorterStemmer, WordNetLemmatizerimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport itertoolsimport pickleimport collectionsfrom sklearn.model_selection  import train_test_split, cross_val_score, GridSearchCVfrom sklearn.metrics import confusion_matrixfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.pipeline import Pipelinefrom sklearn.decomposition import TruncatedSVD, PCAfrom sklearn.linear_model import LogisticRegression# In[2]:def clean(text):     txt = str(text)    txt = re.sub(r'[^A-Za-z\s]',r' ',txt)        st = PorterStemmer()    txt = " ".join([st.stem(w) for w in txt.split()])        wordnet_lemmatizer = WordNetLemmatizer()    txt = " ".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])    return txt# In[3]:def plot_confusion_matrix(cm, classes,                          normalize=False,                          title='Confusion matrix',                          cmap=plt.cm.Blues):        if normalize:        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]    plt.figure(figsize=(8, 6))    plt.imshow(cm, interpolation='nearest', cmap=cmap)    plt.title(title)    plt.colorbar()    tick_marks = np.arange(len(classes))    plt.xticks(tick_marks, classes, rotation=90)    plt.yticks(tick_marks, classes)    fmt = '.2f' if normalize else 'd'    thresh = cm.max() / 2.    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):        plt.text(j, i, format(cm[i, j], fmt),                 horizontalalignment="center",                 color="white" if cm[i, j] > thresh else "black")    plt.ylabel('True label')    plt.xlabel('Predicted label')    plt.tight_layout()# In[4]:files = [name for name in os.listdir("/home/software/Documents/ML-fax-Model/data")]test_data = []for i in range(len(files)):    if files[i] == ".DS_Store":        continue    with open('/home/software/Documents/ML-fax-Model/data/'+files[i], 'rb') as handle:        test_data = test_data + pickle.load(handle)# In[5]:number_of_jobs = 4labels = pd.read_csv("labels.csv")# In[6]:classes = list(map(lambda x : x['class_name'], test_data))labels_1, values = zip(*collections.Counter(classes).items())indexes = np.arange(len(labels_1))width = 1.0# In[8]:for i in range(len(labels_1)):    print("%s = %s"%(labels_1[i], values[i]))# In[9]:indices = [i for i,v in enumerate(values) if v > 50]labels_with_50_instances = [labels_1[i] for i in indices]# In[10]:test_data = list(filter(lambda x : x['class_name'] in labels_with_50_instances, test_data))# In[11]:classes = list(map(lambda x : x['class_name'], test_data))labels_1, values = zip(*collections.Counter(classes).items())indexes = np.arange(len(labels_1))width = 1.0# In[12]:plt.bar(indexes, values, width, align='edge')plt.xticks(indexes + width * 0.5, labels_1, rotation='vertical')#plt.show()# In[13]:pd_data = pd.DataFrame(data=None, columns=["content", "page_count", "label"])for i in range(len(test_data)):    data = test_data[i]    #print(data)    new_data = []    content = "";    page_count = 0    label = data["label"]    #print(data["pages"])    if(data["is_cover_page"]==0):        #print(data["pages"])        for j in range(len(data["pages"])):            content = content + data["pages"][j]        page_count = len(data["pages"])    if(data["is_cover_page"]==1):        #print(data["pages"])        for j in range(1,len(data["pages"])):            content = content + data["pages"][j]        page_count = len(data["pages"])-1    #print(content )    pd_data = pd_data.append({'content': content, "page_count": page_count, "label": label}, ignore_index=True)    # In[14]:X_train, X_test, Y_train, Y_test = train_test_split(pd_data["content"].values.astype('U'), pd_data["label"].values.astype('int64'), test_size=0.10)#print('X_train : '+str(X_train))# In[15]:spacy_nlp = spacy.load('en_core_web_sm')spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS# In[23]:logistic_regression_steps = [                        ("tfidf_vect",TfidfVectorizer()),                        ("svd",TruncatedSVD()),                        ("regressor",LogisticRegression())]logistic_regression_pipeline = Pipeline(logistic_regression_steps)# In[24]:logistic_regression_parameter_grid = {"tfidf_vect__stop_words" : [spacy_stopwords],                   "tfidf_vect__lowercase" : [True],                   "tfidf_vect__token_pattern" : [r'\b[a-zA-Z]{3,}\b'],                  "tfidf_vect__max_features" : [3000],                  "tfidf_vect__ngram_range" : [(1,1)],                  "svd__n_components" : [1500],                  "regressor__multi_class": ["multinomial"],                   "regressor__solver" : ["lbfgs" ],                    "regressor__class_weight": ["balanced"]                }# In[25]:logistic_regression_search = GridSearchCV(logistic_regression_pipeline, logistic_regression_parameter_grid, iid=False, cv=5, return_train_score=False, verbose=20, n_jobs=number_of_jobs)logistic_regression_search = logistic_regression_search.fit(X_train, list(Y_train))# In[ ]:print("Best parameter (CV score=%0.3f):" % logistic_regression_search.best_score_)# In[ ]:logistic_regression_Y_pred = logistic_regression_search.predict(X_test)accuracy = np.mean(logistic_regression_Y_pred==Y_test)*100print("Test Accuracy: %.2f"%(accuracy))# In[ ]:index = np.sort(np.unique(Y_test))classes = np.asarray(labels[labels["index"].isin(index)]["name"])# In[ ]:LR_confusion_matrix = confusion_matrix(Y_test, logistic_regression_Y_pred)# In[26]:plot_confusion_matrix(LR_confusion_matrix, classes, normalize=False ,title='Confusion matrix',cmap=plt.cm.Greens)# In[37]:models_stat = pd.DataFrame(columns = ["Name", "TF-IDF Max Features", "TF-IDF Ngram", "SVD Components", "No. Of Estimators", "Depth", "Accuracy"])for i in range(len(logistic_regression_search.cv_results_["mean_test_score"])):    models_stat = models_stat.append({            "Name":"Logistic Regression",             "TF-IDF Max Features":logistic_regression_search.cv_results_["param_tfidf_vect__max_features"][i],             "TF-IDF Ngram":logistic_regression_search.cv_results_["param_tfidf_vect__ngram_range"][i],             "SVD Components":logistic_regression_search.cv_results_["param_svd__n_components"][i],            "Accuracy":logistic_regression_search.cv_results_["mean_test_score"][i]    },ignore_index=True)# In[ ]:models_stat.sort_values(["Accuracy"], ascending=False)# In[27]:import picklepickle.dump(logistic_regression_search.best_estimator_, open("bestmodel2.mod", "wb"))# In[ ]:labels# In[ ]: